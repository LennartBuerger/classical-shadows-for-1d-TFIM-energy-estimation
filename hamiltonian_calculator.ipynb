{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfermion import MolecularData\n",
    "from openfermion.ops import QubitOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_to_geometry = {\n",
    "    'H2': [['H', [0, 0, 0]],\n",
    "           ['H', [0, 0, 0.734]]],\n",
    "    'LiH': [['Li', [0, 0, 0]],\n",
    "            ['H', [0, 0, 1.548]]],\n",
    "    'NH3': [['N', [0, 0, 0.149]],\n",
    "            ['H', [0, 0.947, -0.348]],\n",
    "            ['H', [0.821, -0.474, -0.348]],\n",
    "            ['H', [-0.821, -0.474, -0.348]]],\n",
    "    'C2': [['C', [0, 0, 0]],\n",
    "           ['C', [0, 0, 1.26]]],\n",
    "    'N2': [['N', [0, 0, 0]],\n",
    "           ['N', [0, 0, 1.19]]],\n",
    "    'H2O': [['H', [0, 0.769, -0.546]],\n",
    "            ['H', [0, -0.769, -0.546]],\n",
    "            ['O', [0, 0, 0.137]]],\n",
    "    'hydrazine': [['N', (0, 0, 1.26135)],\n",
    "                  ['N', (0, 0, -1.26135)],\n",
    "                  ['H', (0, 1.7439, 2.33889)],\n",
    "                  ['H', (0, -1.7439, 2.33889)],\n",
    "                  ['H', (0, 1.7439, -2.33889)],\n",
    "                  ['H', (0, -1.7439, -2.33889)]]\n",
    "}\n",
    "\n",
    "mol_to_spin = {\n",
    "    'H2': 1,\n",
    "    'LiH': 1,\n",
    "    'NH3': 1,\n",
    "    'N2': 1,\n",
    "    'O2': 3,\n",
    "    'H2O': 1,\n",
    "}\n",
    "\n",
    "mol_to_electron_num = {\n",
    "    'H2': 0,\n",
    "    'LiH': 0,\n",
    "    'NH3': 0,\n",
    "    'N2': 0,\n",
    "    'O2': 0,\n",
    "    'H2O': 0,\n",
    "}\n",
    "\n",
    "DEFAULT_MOLECULE_ROOT = './molecules'\n",
    "DEFAULT_MOLECULE_FILENAME = 'molecular_data.hdf5' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the molecule and its main properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mol_filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22110/3249268694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mmol_to_spin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mmol_to_electron_num\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmol_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     filename=mol_filename)\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mol_filename' is not defined"
     ]
    }
   ],
   "source": [
    "mol_name = 'H2'\n",
    "molecule_filename = os.path.join(molecule_dir, DEFAULT_MOLECULE_FILENAME)\n",
    "mol = MolecularData(mol_to_geometry[mol_name],\n",
    "                    'sto-3g',\n",
    "                    mol_to_spin[mol_name],\n",
    "                    mol_to_electron_num[mol_name],\n",
    "                    filename=mol_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For H2O molecule .hdf5 was found, loading the molecule...\n",
      "The molecule was successfully loaded!\n",
      "Hartree-Fock energy: -74.96254759321404\n",
      "FCI energy: -75.02329149983605\n",
      "FCI energy up to chemical accuracy: -75.02169149983605\n"
     ]
    }
   ],
   "source": [
    "mol, mol_dir, of_ham, logger = load_molecule(molecule_name='H2O',\n",
    "                                             create_hamiltonian=True)\n",
    "print(f'Hartree-Fock energy: {mol.hf_energy}')\n",
    "print(f'FCI energy: {mol.fci_energy}')\n",
    "print(f'FCI energy up to chemical accuracy: {mol.fci_energy + CHEMICAL_ACCURACY}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible_num = mol.n_orbitals * 2\n",
    "electron_num = mol.n_electrons\n",
    "spin = (mol.multiplicity - 1) // 2\n",
    "\n",
    "hf_idx = (2 ** electron_num - 1) << (visible_num - electron_num)\n",
    "\n",
    "cpu_device = pt.device('cpu')\n",
    "gpu_device = pt.device('cuda:0')\n",
    "work_device = gpu_device if pt.cuda.is_available() else cpu_device\n",
    "work_dtype = BASE_COMPLEX_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining Hamiltonians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamiltonians exist on the disk, we load them\n",
      "MPS None:\n",
      "\tvisible_num = 14\n",
      "\tphys_dims = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "\tbond_dims = [4, 16, 33, 54, 69, 60, 77, 92, 71, 46, 33, 16, 4]\n",
      "\text_bond_dims = [1, 4, 16, 33, 54, 69, 60, 77, 92, 71, 46, 33, 16, 4, 1]\n",
      "\torth_idx = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ham_mps_num = 1\n",
    "ham_mpses = []\n",
    "ham_mpses_dir = os.path.join(mol_dir,\n",
    "                             f'{ham_mps_num}_ham_mpses_{work_dtype}')\n",
    "if os.path.exists(ham_mpses_dir):\n",
    "    print(f'Hamiltonians exist on the disk, we load them')\n",
    "    for ham_idx in range(ham_mps_num):\n",
    "        tensor_list = []\n",
    "        for idx in range(visible_num):\n",
    "            tensor_list.append(pt.from_numpy(np.load(os.path.join(ham_mpses_dir,\n",
    "                                                                  f'ham_mps_{ham_idx}_tensor_{idx}.npy'))))\n",
    "        ham_mpses.append(MPS.from_tensor_list(tensor_list))\n",
    "        print(ham_mpses[-1])\n",
    "else:\n",
    "    print(f'Hamiltonians do not exist on the disk, you have to create them manually!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Hamiltonians if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pauli_strings_to_terms(pauli_strings):\n",
    "    terms = []\n",
    "    for pauli_string in pauli_strings:\n",
    "        term = ()\n",
    "        for qubit_idx, pauli_char in enumerate(pauli_string[::-1]):\n",
    "            if pauli_char != 'I':\n",
    "                term += (tuple((qubit_idx, pauli_char)),)\n",
    "        terms.append(term)\n",
    "        \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pauli_strings_to_of_ham(pauli_strings, parent_of_ham):\n",
    "    weighted_terms = {}\n",
    "    terms = pauli_strings_to_terms(pauli_strings)\n",
    "    weighted_terms.update({\n",
    "        term: parent_of_ham.terms[term] for term in terms\n",
    "    })\n",
    "    of_ham = 0.0 * QubitOperator(' ')\n",
    "    of_ham.compress()\n",
    "    of_ham.terms = weighted_terms\n",
    "    \n",
    "    return ham   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_list_to_tree_ham_mps(node_list, parent_of_ham, dtype, verbose=False):\n",
    "    weighted_terms = {}\n",
    "    for node in node_list:\n",
    "        pauli_strings = node.get_pauli_strings()\n",
    "        terms = pauli_strings_to_terms(pauli_strings)\n",
    "        weighted_terms.update({\n",
    "            term: parent_of_ham.terms[term] for term in terms\n",
    "        })\n",
    "    of_ham = 0.0 * QubitOperator(' ')\n",
    "    of_ham.compress()\n",
    "    of_ham.terms = weighted_terms\n",
    "    tree_ham = TreeHamiltonian(qubit_num=visible_num,\n",
    "                               of_hamiltonian=of_ham,\n",
    "                               dtype=dtype)\n",
    "    tree_ham.compress(verbose=verbose)\n",
    "    \n",
    "    tree_ham_mps = tree_ham.to_mps()\n",
    "    tree_ham_mps.trim_bond_dims()\n",
    "    \n",
    "    return tree_ham_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing of OF QubitOperator started...\n",
      "Finished!\n",
      "MPS None:\n",
      "\tvisible_num = 14\n",
      "\tphys_dims = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "\tbond_dims = [4, 16, 33, 54, 69, 60, 77, 92, 71, 46, 33, 16, 4]\n",
      "\text_bond_dims = [1, 4, 16, 33, 54, 69, 60, 77, 92, 71, 46, 33, 16, 4, 1]\n",
      "\torth_idx = 12\n",
      "\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "tree_ham = TreeHamiltonian(qubit_num=visible_num,\n",
    "                           of_hamiltonian=of_ham,\n",
    "                           dtype=work_dtype)\n",
    "tree_ham.compress(verbose=False)\n",
    "tree_ham_mps = tree_ham.to_mps()\n",
    "tree_ham_mps.trim_bond_dims()\n",
    "print(tree_ham_mps)\n",
    "print(max(tree_ham_mps.bond_dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose level of tree at which we want to perform splitting (presumably the closer to the middle, the more fine-grained are the partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node #0\n",
      "Number of terms: 214\n",
      "\n",
      "Node #1\n",
      "Number of terms: 54\n",
      "\n",
      "Node #2\n",
      "Number of terms: 54\n",
      "\n",
      "Node #3\n",
      "Number of terms: 54\n",
      "\n",
      "Node #4\n",
      "Number of terms: 54\n",
      "\n",
      "Node #5\n",
      "Number of terms: 42\n",
      "\n",
      "Node #6\n",
      "Number of terms: 42\n",
      "\n",
      "Node #7\n",
      "Number of terms: 42\n",
      "\n",
      "Node #8\n",
      "Number of terms: 42\n",
      "\n",
      "Node #9\n",
      "Number of terms: 22\n",
      "\n",
      "Node #10\n",
      "Number of terms: 22\n",
      "\n",
      "Node #11\n",
      "Number of terms: 22\n",
      "\n",
      "Node #12\n",
      "Number of terms: 22\n",
      "\n",
      "Node #13\n",
      "Number of terms: 22\n",
      "\n",
      "Node #14\n",
      "Number of terms: 20\n",
      "\n",
      "Node #15\n",
      "Number of terms: 20\n",
      "\n",
      "Node #16\n",
      "Number of terms: 10\n",
      "\n",
      "Node #17\n",
      "Number of terms: 10\n",
      "\n",
      "Node #18\n",
      "Number of terms: 10\n",
      "\n",
      "Node #19\n",
      "Number of terms: 10\n",
      "\n",
      "Node #20\n",
      "Number of terms: 10\n",
      "\n",
      "Node #21\n",
      "Number of terms: 10\n",
      "\n",
      "Node #22\n",
      "Number of terms: 10\n",
      "\n",
      "Node #23\n",
      "Number of terms: 10\n",
      "\n",
      "Node #24\n",
      "Number of terms: 3\n",
      "\n",
      "Node #25\n",
      "Number of terms: 12\n",
      "\n",
      "Node #26\n",
      "Number of terms: 12\n",
      "\n",
      "Node #27\n",
      "Number of terms: 3\n",
      "\n",
      "Node #28\n",
      "Number of terms: 6\n",
      "\n",
      "Node #29\n",
      "Number of terms: 6\n",
      "\n",
      "Node #30\n",
      "Number of terms: 6\n",
      "\n",
      "Node #31\n",
      "Number of terms: 6\n",
      "\n",
      "Node #32\n",
      "Number of terms: 6\n",
      "\n",
      "Node #33\n",
      "Number of terms: 6\n",
      "\n",
      "Node #34\n",
      "Number of terms: 6\n",
      "\n",
      "Node #35\n",
      "Number of terms: 6\n",
      "\n",
      "Node #36\n",
      "Number of terms: 3\n",
      "\n",
      "Node #37\n",
      "Number of terms: 3\n",
      "\n",
      "Node #38\n",
      "Number of terms: 6\n",
      "\n",
      "Node #39\n",
      "Number of terms: 6\n",
      "\n",
      "Node #40\n",
      "Number of terms: 3\n",
      "\n",
      "Node #41\n",
      "Number of terms: 3\n",
      "\n",
      "Node #42\n",
      "Number of terms: 12\n",
      "\n",
      "Node #43\n",
      "Number of terms: 12\n",
      "\n",
      "Node #44\n",
      "Number of terms: 3\n",
      "\n",
      "Node #45\n",
      "Number of terms: 3\n",
      "\n",
      "Node #46\n",
      "Number of terms: 3\n",
      "\n",
      "Node #47\n",
      "Number of terms: 3\n",
      "\n",
      "Node #48\n",
      "Number of terms: 3\n",
      "\n",
      "Node #49\n",
      "Number of terms: 3\n",
      "\n",
      "Node #50\n",
      "Number of terms: 3\n",
      "\n",
      "Node #51\n",
      "Number of terms: 3\n",
      "\n",
      "Node #52\n",
      "Number of terms: 3\n",
      "\n",
      "Node #53\n",
      "Number of terms: 3\n",
      "\n",
      "Node #54\n",
      "Number of terms: 3\n",
      "\n",
      "Node #55\n",
      "Number of terms: 3\n",
      "\n",
      "Node #56\n",
      "Number of terms: 3\n",
      "\n",
      "Node #57\n",
      "Number of terms: 3\n",
      "\n",
      "Node #58\n",
      "Number of terms: 3\n",
      "\n",
      "Node #59\n",
      "Number of terms: 3\n",
      "\n",
      "Node #60\n",
      "Number of terms: 3\n",
      "\n",
      "Node #61\n",
      "Number of terms: 3\n",
      "\n",
      "Node #62\n",
      "Number of terms: 3\n",
      "\n",
      "Node #63\n",
      "Number of terms: 3\n",
      "\n",
      "Node #64\n",
      "Number of terms: 3\n",
      "\n",
      "Node #65\n",
      "Number of terms: 3\n",
      "\n",
      "Node #66\n",
      "Number of terms: 3\n",
      "\n",
      "Node #67\n",
      "Number of terms: 3\n",
      "\n",
      "Node #68\n",
      "Number of terms: 3\n",
      "\n",
      "Node #69\n",
      "Number of terms: 3\n",
      "\n",
      "Node #70\n",
      "Number of terms: 1\n",
      "\n",
      "Node #71\n",
      "Number of terms: 1\n",
      "\n",
      "Node #72\n",
      "Number of terms: 2\n",
      "\n",
      "Node #73\n",
      "Number of terms: 2\n",
      "\n",
      "Node #74\n",
      "Number of terms: 1\n",
      "\n",
      "Node #75\n",
      "Number of terms: 1\n",
      "\n",
      "Node #76\n",
      "Number of terms: 1\n",
      "\n",
      "Node #77\n",
      "Number of terms: 1\n",
      "\n",
      "Node #78\n",
      "Number of terms: 1\n",
      "\n",
      "Node #79\n",
      "Number of terms: 1\n",
      "\n",
      "Node #80\n",
      "Number of terms: 1\n",
      "\n",
      "Node #81\n",
      "Number of terms: 1\n",
      "\n",
      "Node #82\n",
      "Number of terms: 1\n",
      "\n",
      "Node #83\n",
      "Number of terms: 1\n",
      "\n",
      "Node #84\n",
      "Number of terms: 1\n",
      "\n",
      "Node #85\n",
      "Number of terms: 1\n",
      "\n",
      "Node #86\n",
      "Number of terms: 1\n",
      "\n",
      "Node #87\n",
      "Number of terms: 1\n",
      "\n",
      "Node #88\n",
      "Number of terms: 1\n",
      "\n",
      "Node #89\n",
      "Number of terms: 1\n",
      "\n",
      "Node #90\n",
      "Number of terms: 1\n",
      "\n",
      "Node #91\n",
      "Number of terms: 1\n",
      "\n",
      "Node #92\n",
      "Number of terms: 1\n",
      "\n",
      "Node #93\n",
      "Number of terms: 1\n",
      "\n",
      "Node #94\n",
      "Number of terms: 1\n",
      "\n",
      "Node #95\n",
      "Number of terms: 1\n",
      "\n",
      "Node #96\n",
      "Number of terms: 1\n",
      "\n",
      "Node #97\n",
      "Number of terms: 1\n",
      "\n",
      "Node #98\n",
      "Number of terms: 1\n",
      "\n",
      "Node #99\n",
      "Number of terms: 1\n",
      "\n",
      "Node #100\n",
      "Number of terms: 1\n",
      "\n",
      "Node #101\n",
      "Number of terms: 1\n",
      "\n",
      "Node #102\n",
      "Number of terms: 1\n",
      "\n",
      "Node #103\n",
      "Number of terms: 1\n",
      "\n",
      "Node #104\n",
      "Number of terms: 1\n",
      "\n",
      "Node #105\n",
      "Number of terms: 1\n",
      "\n",
      "Node #106\n",
      "Number of terms: 1\n",
      "\n",
      "Node #107\n",
      "Number of terms: 1\n",
      "\n",
      "Node #108\n",
      "Number of terms: 1\n",
      "\n",
      "Node #109\n",
      "Number of terms: 1\n",
      "\n",
      "Node #110\n",
      "Number of terms: 1\n",
      "\n",
      "Node #111\n",
      "Number of terms: 1\n",
      "\n",
      "Node #112\n",
      "Number of terms: 1\n",
      "\n",
      "Node #113\n",
      "Number of terms: 1\n",
      "\n",
      "Node #114\n",
      "Number of terms: 1\n",
      "\n",
      "Node #115\n",
      "Number of terms: 1\n",
      "\n",
      "Node #116\n",
      "Number of terms: 1\n",
      "\n",
      "Node #117\n",
      "Number of terms: 1\n",
      "\n",
      "Node #118\n",
      "Number of terms: 1\n",
      "\n",
      "Node #119\n",
      "Number of terms: 1\n",
      "\n",
      "Node #120\n",
      "Number of terms: 1\n",
      "\n",
      "Node #121\n",
      "Number of terms: 1\n",
      "\n",
      "Total number of terms: 1086\n"
     ]
    }
   ],
   "source": [
    "split_layer_idx = 5\n",
    "term_nums = pt.zeros((len(tree_ham.tree[split_layer_idx]), ), dtype=pt.int)\n",
    "for node_idx, node in enumerate(tree_ham.tree[split_layer_idx]):\n",
    "    print(f'Node #{node_idx}')\n",
    "    cur_term_num = len(node.get_pauli_strings())\n",
    "    print(f'Number of terms: {cur_term_num}\\n')\n",
    "    term_nums[node_idx] = cur_term_num\n",
    "print(f'Total number of terms: {term_nums.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms per MPS: 1086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24222/1519806481.py:2: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  terms_per_mps = term_nums.sum() // ham_mpses_num\n"
     ]
    }
   ],
   "source": [
    "ham_mpses_num = 1\n",
    "terms_per_mps = term_nums.sum() // ham_mpses_num\n",
    "print(f'Terms per MPS: {terms_per_mps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = [[] for part_idx in range(ham_mpses_num)]\n",
    "part_term_nums = pt.zeros((ham_mpses_num, ), dtype=pt.int)\n",
    "cur_part_idx = 0\n",
    "for node_idx, node in enumerate(tree_ham.tree[split_layer_idx]):\n",
    "    cur_term_num = len(node.get_pauli_strings())\n",
    "    partitions[cur_part_idx].append(node)\n",
    "    part_term_nums[cur_part_idx] += cur_term_num\n",
    "    if (part_term_nums[cur_part_idx] + cur_term_num > terms_per_mps) and (cur_part_idx != ham_mpses_num - 1):\n",
    "        cur_part_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE YOU CAN REVAMP PARTITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing of OF QubitOperator started...\n",
      "Finished!\n",
      "MPS None:\n",
      "\tvisible_num = 14\n",
      "\tphys_dims = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "\tbond_dims = [4, 16, 33, 54, 69, 60, 77, 92, 71, 46, 33, 16, 4]\n",
      "\text_bond_dims = [1, 4, 16, 33, 54, 69, 60, 77, 92, 71, 46, 33, 16, 4, 1]\n",
      "\torth_idx = 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_ham_mpses = []\n",
    "for partition in partitions:\n",
    "    tree_ham_mpses.append(node_list_to_tree_ham_mps(partition, of_ham, dtype=work_dtype))\n",
    "    print(tree_ham_mpses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_mpses_dir = os.path.join(mol_dir,\n",
    "                             f'{ham_mps_num}_ham_mpses_{work_dtype}')\n",
    "\n",
    "if not os.path.exists(ham_mpses_dir):\n",
    "    os.makedirs(ham_mpses_dir)\n",
    "    \n",
    "ham_mpses = tree_ham_mpses\n",
    "assert len(ham_mpses) == ham_mpses_num\n",
    "for ham_idx, ham_mps in enumerate(ham_mpses):\n",
    "    for idx in range(visible_num):\n",
    "        np.save(os.path.join(ham_mpses_dir,\n",
    "                             f'ham_mps_{ham_idx}_tensor_{idx}.npy'),\n",
    "                ham_mps.tensors[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
